# 5. Logstash

<br>

- 비츠 에이전트나 Syslog 스트림과 같은 소스의 데이터를 수신하는 역할
- HTTP로 데이터를 수신할 수 있어 다양한 소스 시스템으로 이벤트를 수신할 수 있다
- 데이터를 가공하여 다른 대상 시스템으로 데이터를 로드하는 독립형 컴포넌트

<br>

## Logstash 개념들
- **로그스태시 인스턴스** : 실행중인 로그스태시 프로세스
- **파이프라인** : 지정된 워크로드를 처리하기 위해 구성된 플러그인의 모음
- **입력 플러그인** 지정된 소스 시스템에서 데이터를 추출하거나 수신하는데 사용
- **필터 플러그인** : 이벤트를 변환하고 보강하는데 사용
- **출력 플러그인** : 지정된 대상 시스템으로 데이터를 로드하거나 전송하는 데 사용

<br>

<br>

## 예제

설치후 logstash 명령어로 바로 실행되게 하기 (PATH 등록)
```bash
sudo ln -s /usr/share/logstash/bin/logstash /usr/local/bin/logstash
```

<br>

logstash-pipeline.conf 파일
```
input {
    stdin {}
}

filter {
    mutate {
        add_field => { "description" => "First pipeline!" }
    }
}

output {
    stdout {}
}
```

<br>

실행
```
echo -e "Hello world!" | logstash -f $(pwd)/logstash-pipeline.conf
```

<br>

결과
```
{
        "message" => "Hello world!",
     "@timestamp" => 2025-07-16T03:20:44.797328468Z,
       "@version" => "1",
          "event" => {
        "original" => "Hello world!"
    },
    "description" => "First pipeline!",
           "host" => {
        "hostname" => "DESKTOP-L9BQF29"
    }
}
```

<br>

## 입력 플러그인
- `stdin` input : Logstash에서 사용하는 input 플러그인 중 하나로, 표준 입력(terminal 입력, 파이프 등) 으로부터 데이터를 받아오는 기능
    - 배포 환경에서는 사용권장이 되지 않는다 -> beats와 연동해서 쓰기
- `file` input : Logstash가 로컬 파일(CSV, 로그 등) 을 한 줄씩 읽어들이는 역할
    - 경로는 무조건 **절대경로**

### `sincedb`란
> Logstash가 파일을 읽을 때, **어디까지 읽었는지 기억**하기 위해 저장하는 파일

- Logstash가 csv 같은 파일을 읽으면
- 마지막에 읽은 위치(offset)를 sincedb라는 내부 파일에 기록
- Logstash를 재시작해도 이전에 어디까지 읽었는지 알아서 이어서 처리

<br>

**테스트 환경 에서는**
- sample.csv 를 Logstash가 한 번 읽음 → sincedb에 위치 기록
- 파일을 수정하지 않고 다시 실행하면 → "변경 없음" → 다시 읽지 않음
    - 때문에
    - `sincedb_clean_after` 쓴다
    - ```
       sincedb_clean_after => "1 s"
      ```
    - 최근에 관찰된 이후 1초가 지나면 이 sincedb 정보는 더 이상 유효하지 않다고 간주하고 지워버려라

<br>

**운영환경에서는**

sincedb가 이전 처리 이력을 기억하고 있어야:
- 로그 중복 처리 ❌
- 성능 낭비 ❌
- 결과 중복 ❌

때문에 `sincedb_clean_after` 제거

<br><br>

## 필터 플러그인
- `csv` 필터 : CSV 형식의 데이터를 구조화된 JSON 필드로 파싱할 때 사용
    - 매개변수
    - `skip_header`: 파일의 첫 번쩨 줄을 건너뛰도록 플러그인에 지시
    - `columns` : CSV 열 이름을 파일에 보이는 순서대로 정의
- `dissect` 필터 : 정형적인 문자열 포맷을 빠르게 분리할 수 있도록 도와주는 Logstash 필터
- `mutate` 필터 : 이벤트의 필드값을 수정하거나 가공할 때 사용
    - 매개변수
    - `gsub` : 정규표현식 패턴을 사용해 일치하는 항목을 다른 문자열로 대체
    - `remove_field` : 이벤트에서 필드를 제거하는 옵션
- `date` 필터 : 타임스탬프 값을 유효한 형식으로 변환
    - 매개변수
    - `match` : 사용되는 필드 이름과 타임스탬프 형식을 정의
    - `target` : 변환 후 저장할 필드를 정의
- `grok` 필터 : 비정형 텍스트(주로 로그)를 정형화된 구조(JSON 등)로 파싱할 때 사용

<br><br>

## 출력 플러그인
- `stdout` 출력 : 이벤트를 콘솔에 출력
- `elasticsearch` 출력 : Logstash가 처리한 데이터를 Elasticsearch 인덱스에 저장